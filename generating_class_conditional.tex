%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% From a template maintained at https://github.com/jamesrobertlloyd/cbl-tikz-poster
%
% Code near the top should be fairly standard and not need to be changed
%  - except for the document class
% Code lower down is more likely to be customised
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pdfminorversion=4
\documentclass[portrait,a0b,final,a4resizeable]{include/a0poster}


\usepackage{multicol}
\usepackage{color}
\usepackage{morefloats}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}


\usepackage{include/picins}
\usepackage{tikz}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
% \usepackage{algorithmic}
% \algsetup{linenosize=\small}
\usepackage{setspace}
\usepackage[hypcap=false,font=small,labelfont=bf]{caption} 
\usepackage{makecell}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{blue}{rgb}{0,0,1}



\usepackage{nicefrac}
\newcommand{\vect}[1]{\underline{\smash{#1}}}
\renewcommand{\v}[1]{\vect{#1}}
\newcommand{\reals}{\mathds{R}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\br}{}%{^{\text{\textnormal{ r}}}}
\newcommand{\cat}{^{\text{\textnormal{c}}}}

\usepackage{dsfont}
\usepackage{apacite}

\input{include/preamble.sty}

\input{include/jlposter.tex}


\newcommand\transpose{{\textrm{\tiny{\sf{T}}}}}
\newcommand{\note}[1]{}
\newcommand{\hlinespace}{~\vspace*{-0.15cm}~\\\hline\\\vspace*{0.15cm}}
\newcommand{\embeddingletter}{g}
\newcommand{\bo}{{\sc bo}}
%\newcommand{\gp}{{\sc gp}}
\newcommand{\agp}{Arc \gp}



\begin{document}
\begin{poster}

% Potentially add some space at the top of the poster
\vspace{0\baselineskip}



%%% Header
\begin{center}
\begin{pcolumn}{0.99}

\newcommand{\logowidth}{0.06\textwidth}  % width mauna decomp

\pbox{0.99\textwidth}{}{linewidth=2mm,framearc=0.3,linecolor=camdarkblue,fillstyle=gradient,gradangle=0,gradbegin=white,gradend=white,gradmidpoint=1.0,framesep=1em}{
%%% U Toronto Logo
\hspace{2.3cm}
\begin{minipage}[c][][b]{\logowidth}
%  \begin{center}
    \vspace{-.18in}
    \includegraphics[width=7cm,trim=9em 0em 9em 0em, clip]{badges/toronto}
   	% \hspace{35cm}
     \includegraphics[width=7cm]{badges/uot_text.png}
\end{minipage}
%
%%% Title
\hspace{-5.0cm}
\begin{minipage}[c][9cm][c]{0.94\textwidth}
  \begin{center}
    \vspace{1.5cm}
    {\sffamily \Huge \textbf{Fast Weights Using Improved Memory Consolidation Designs}}\\[7.5mm]
    {\huge\sffamily Bowen Xu$^1$, Jimmy Ba$^2$, Richard Zemel$^1$\\[7.5mm]
  	\Large\texttt{Department of Computer Science, University of Toronto$^1$} \\[5mm]
  	\Large\texttt{Department of Electrical \& Computer Engineering, University of Toronto$^2$}
    }
  \end{center}
\end{minipage}
}
\end{pcolumn}
\end{center}

\vspace*{2.5cm}

\large


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% Begin of Multicols-Enviroment
\begin{multicols}{3}
% 
\mysection{Abstract}
%
% 
\vspace{0.2in}
% 
{
\Large\sffamily
Storing better quality and greater amount of memory has been a difficult challenge for deep learning.
While the current artificial neural networks excel at tasks such as regression and classification, they fail to perform equally well on representing variables and storing data over long time.
We introduce an enhanced fast weights model that includes robust memory consolidation designs into the existing memory transformation mechanisms.
We show that our model comes with no additional costs, converges faster, and out-performs the original fast weights model on the associative retrieval tasks.
}
\begin{quote}
\vspace{0.2in}
\textbf{Keywords:} 
Fast Weights; Memory Consolidation; Memory Transformation; Deep Learning; Associative Retrieval
\end{quote}

\vspace{0.5in}

\mysection{Background}
%
Recently, Benna and Fusi have demonstrated that the combination of a power-law memory decay with a fast new memory adaptability maximizes both the memory storage and lifetime (2016).
Their method treats each piece of memory as random and uncorrelated, and maximizes the signal to noise ratio (SNR) of all the stored memories \cite{pld}.
Since Benna and Fusi use a power-law decay function which has the form of \emph{$t^{-\gamma}$}, $\gamma > 0.5$ is required for the function to converge (2016).
In addition, Benna and Fusi also suggest fast adaptability for the memory system which implies that the learning rate for new information should be exactly $1.0$ (2016).
% 
\vspace{0.25in}
\begin{align}
    &w_a(t) \equiv \sum_{t^{'} < t} \Delta w_a(t') r(t-t')
    \label{eq:memory} \\
    &S_{t^{'}}(t) \equiv \frac{1}{N}\left<\sum_{a=1}^N w_a(t)\Delta w_a(t') \right>
    \label{eq:signal} \\
    &N^2_{t^{'}}(t) \equiv \left<\frac{1}{N^2}\left(\sum_{a=1}^N w_a(t)\Delta w_a(t') \right)^2 \right> - S^2_{t^{'}}(t)
    \label{eq:variance} \\
    &S / N(t - t') = \sqrt{\frac{N r^2(t - t')}{\sum_{t" < t, t" \neq t'} r^2(t- t")}}
    \label{eq:snr}\\
    &\sum_{t" < t, t" \neq t'} r^2(t- t") \approx \int_{1}^{\infty} r^2(t) dt 
    \label{eq:decay}
\end{align}
\vspace{0.25in}

Recent advances in deep learning \cite{dnc, fw} have also shown that the use of an external memory with the artificial neural networks can significantly improve their abilities to retain information.
Ba et.\ al.\ devise a bidirectional memory transformation system.
Knowledge is not only transferred from the fast weights to the memory cells of a Recurrent Neural Network (RNN) through its hidden vector update but also transitioned from the memory cells to the fast weights as it is updated using the latest hidden vector at each time step.
Also, a learning rate $\eta$ and a decay rate $\lambda$ are included to control how much the fast weights should store new knowledge versus how quickly it should forget previous knowledge.
%
\vspace{0.25in}
\begin{align}
    &A(t) = \lambda A(t-1) + \eta h(t) h(t)^T
    \label{eq:external_memory} \\
    &h_s(t+1) = f(LN(Wh(t) + Cx(t) + A(t)h_{s-1}(t+1)))
    \label{eq:hidden_update} \\
    &h(t+1) = h_s(t+1) \label{eq:hidden_assign}
\end{align}
\vspace{1cm}

\mysection{Our Contribution}
We incorporate memory consolidation designs from \cite{pld} to the fast weights model \cite{fw}.
We keep the fast weights setup and the weights transferring mechanism unchanged.
However, our fast weights uses a learning rate of $\eta$ $=1.0$ instead of $\eta$ $=0.5$.
We also substitute the original exponential decay function $\lambda$ $=0.95^t$ to a power-law decay function $\prod_{t=1}^{t=\tau} t^{-0.5}$ where $\tau$ is the number of time steps elapsed since the initial storage of a specific memory. 
 

\newpage 
\mysection{Decay Functions Comparison}
 \vspace{-0.05cm}

Figure below shows that our power-law decay function decays much faster than the original exponential decay function. 
We believe that because new information is stored in a greater portion than before, the fast weights has to decay its existing memory quicker to prevent memory interference. 
This design also forces the RNN to learn patterns quicker from the fast weights since existing information is quickly removed.

\vspace{-0.75in}
\begin{align*}
 \includegraphics[width=\columnwidth,keepaspectratio=False]{figures/functions}
\end{align*}
% 
\mysection{Experiments and Results}
We evaluate three RNN models on the associative retrieval tasks on three difficulty levels.
Our data consists of sequences of key-value pairs concatenated with two question marks and one query key.
The keys are lower case characters from the English alphabet chosen randomly without replacement, and the values are digits among 0 to 9 chosen randomly with replacement. \\

\begin{minipage}[c]{1\columnwidth}
        \centering
        \resizebox{!}{!}{
		\begin{tabular}{cc}
		Input String & \makecell{Target} \\
		\midrule
		c9k8j3f1??c & 9 \\
		\midrule
		j0a5s5z2??a & 5 \\
		\end{tabular}}
		\label{table:examples}
\end{minipage}
\vspace{0.25in}

Our training data contains $100,000$ sequences and our testing data contains $50,000$ sequences.
The difficulty levels are three key-value pairs (K = $3$), four key-value pairs (K = $4$), and twenty-six key-value pairs (K = $26$).
The following models are evaluated on correctly predicting the associated value of the query key: a vanilla RNN (CONTROL), the original fast weights model (RNN-LN-FW), and the improved fast weights model (RNN-LN-FW2).
All RNN models contain a single hidden layer with $50$ hidden units and are trained with $100$ iterations.\\

\begin{minipage}[c]{1\columnwidth}
        \centering
        \resizebox{!}{!}{
		\begin{tabular}{cccc}
		Model & \makecell{K = $3$} & \makecell{K = $4$} & \makecell{K = $26$} \\
		\midrule
		RNN-LN-FW2 & {$\bf 99.74\%$} & {$\bf 100\%$} & {$\bf 100\%$} \\
		\midrule
		RNN-LN-FW & $99.5\%$ & $99.71\%$ & $94.75\%$ \\
		\midrule
		\makecell{CONTROL} & $42.71\%$ & $34.49\%$ & $12.07\%$\\
		\end{tabular}}
		\label{table:compare_methods}
\end{minipage}

\vspace{1.25in}
\mycaption{Testing Accuracy K=26}
\vspace{-1.25in}
\myfig{figures/k26}{1}

%\begin{align*}
%\hspace{-0.475in}
%  \includegraphics[width=30cm,keepaspectratio=False]{figures/k26}
%\end{align*}

\newpage
\mysection{Hidden Activations Visualization}
\vspace{-0.5cm}
\begin{align*}
\hspace{-0.475in}
\includegraphics[width=30cm,keepaspectratio=False]{figures/Full_Comparison_RNN-LN-FW2_sample1}
\end{align*}

\vspace{-1.25in}
\begin{align*}
\hspace{-0.475in}
\includegraphics[width=30cm,keepaspectratio=False]{figures/Full_Comparison_RNN-LN-FW_sample3}
\end{align*}


\vspace{1.cm}
\mysection{Conclusion and Future Work}
We introduce an enhanced fast weights model by incorporating memory consolidation designs from \cite{pld}.
We show that our model works better on the associative retrieval tasks and converges faster.
Our next direction is to evaluate the models on more difficult tasks such as sorting and repeated-copying.
We also intend to investigate other decay functions for the fast weights model.

\vspace{1.cm}

\mysection{References}
\small
\begingroup
\renewcommand{\section}[2]{}%
\bibliography{bib}
\endgroup
\bibliographystyle{apacite}

 
\end{multicols}
\end{poster}
\end{document}

